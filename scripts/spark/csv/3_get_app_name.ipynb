{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5cacfe2-af9e-43c0-a954-96dc89632b6a",
   "metadata": {},
   "source": [
    "Objetivo: Ler todos os Games no arquivo .CSV e mapear o nome do Game + Max(ReviewID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce18a2-e43b-404e-8c03-85a35f220d05",
   "metadata": {},
   "source": [
    "<h2>Importando bibliotecas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8755f2b-29f5-411d-91ed-3cd645ae0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b10d3d7-9ec4-4a0a-9a35-4e69ce10e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a338783b-5f04-41e4-8799-abf0962f065b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3564ba65-c498-4061-b1f2-45cbcb88b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, collection\n",
    "import urllib.parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05cca2b7-5f4b-4122-88df-4ba6515d4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/acsantos/Documents/Facens_Architecture-for-Data-Processing/scripts/extract_reviews_from_steam_api')\n",
    "from credentials import credentials\n",
    "from update_last_review import update_last_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15ae563a-e852-4ee1-bfd2-d80f7b660191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4e129-a0f6-4f04-be97-94272a979f49",
   "metadata": {},
   "source": [
    "## Função para listar todos os jogos (appid) mapeados na camada Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66401c72-3921-433f-aaba-dba64bf46938",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9281e7c-874f-44c6-99f7-35ef2e5fb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getGames():\n",
    "\n",
    "    username = urllib.parse.quote_plus(credentials['username'])\n",
    "    password = urllib.parse.quote_plus(credentials['password'])\n",
    "\n",
    "    CONNECTION_STRING = f'mongodb://{username}:{password}@localhost:27017/steam'\n",
    "    client = MongoClient(CONNECTION_STRING)\n",
    "    \n",
    "    appidList = list()\n",
    "\n",
    "    with client:\n",
    "        db = client.steam\n",
    "        games = db.games.find()\n",
    "        \n",
    "        for game in games:\n",
    "            \n",
    "            info = {\n",
    "                'appid' : game['appid'],\n",
    "                'silverPath' : game['silverPath']\n",
    "            }\n",
    "            \n",
    "            appidList.append(info)\n",
    "    \n",
    "    return appidList\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5b9b30-7e9a-456b-95b6-cc1751ff6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "appidList = getGames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a004f1ab-bba3-402b-8787-a2d6351429ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'appid': '0',\n",
       "  'silverPath': 'steam_reviews/reviews.parquet/app_id=0/part-00000-f2af1117-0425-4810-b828-16de4ccbf14b.c000.snappy.parquet'},\n",
       " {'appid': '1',\n",
       "  'silverPath': 'steam_reviews/reviews.parquet/app_id=1/part-00000-f2af1117-0425-4810-b828-16de4ccbf14b.c000.snappy.parquet'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appidList[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc5ffc67-8cd8-4dc7-9192-2a1f289b22ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 3.58 µs\n",
      "65580\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "print(len(appidList))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafddd73-01b2-42da-af78-4408df23840f",
   "metadata": {},
   "source": [
    "<h2> Criando sessão do spark + configurações de conexão com bucket </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb7a61b-cffc-4d7c-a96f-c3f3d176b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/04 22:19:32 WARN Utils: Your hostname, moon resolves to a loopback address: 127.0.1.1; using 192.168.0.185 instead (on interface wlo1)\n",
      "22/04/04 22:19:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/04 22:19:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('ETL - Send to Gold').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d44084-3487-4232-ac3b-6a216ee50320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(spark_context: SparkContext):\n",
    "    spark_context._jsc.hadoopConfiguration().set('fs.s3a.aws.credentials.provider', 'com.amazonaws.auth.EnvironmentVariableCredentialsProvider')\n",
    "    spark_context._jsc.hadoopConfiguration().set('fs.s3a.path.style.access', 'true')\n",
    "    spark_context._jsc.hadoopConfiguration().set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "    spark_context._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 'http://localhost:9000')\n",
    "    spark_context._jsc.hadoopConfiguration().set('fs.s3a.connection.ssl.enabled', 'false')\n",
    "    \n",
    "load_config(spark.sparkContext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838402d-8041-4693-959d-774a82a4855a",
   "metadata": {},
   "source": [
    "<h2> Lendo dados do bucket </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a7f6225-9c1b-40b9-8187-6c2fe650a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews_schema = StructType(\n",
    "    [StructField(\"_c0\", IntegerType(), False),\n",
    "     StructField(\"app_id\", IntegerType(), False),\n",
    "     StructField(\"app_name\", StringType(), False),\n",
    "     StructField(\"review_id\", IntegerType(), False),\n",
    "     StructField(\"language\", IntegerType(), False),\n",
    "     StructField(\"timestamp_created\", IntegerType(), False),\n",
    "     StructField(\"timestamp_updated\", IntegerType(), False),\n",
    "     StructField(\"recommended\", StringType(), False),\n",
    "     StructField(\"votes_helpful\", IntegerType(), False),\n",
    "     StructField(\"votes_funny\", IntegerType(), False),\n",
    "     StructField(\"weighted_vote_score\", IntegerType(), False),\n",
    "     StructField(\"comment_count\", IntegerType(), False),\n",
    "     StructField(\"steam_purchase\", StringType(), False),\n",
    "     StructField(\"received_for_free\", StringType(), False),\n",
    "     StructField(\"written_during_early_access\", StringType(), False),\n",
    "     StructField(\"author.steamid\", IntegerType(), False),\n",
    "     StructField(\"author.num_games_owned\", IntegerType(), False),\n",
    "     StructField(\"author.num_reviews\", IntegerType(), False),\n",
    "     StructField(\"author.playtime_forever\", IntegerType(), False),\n",
    "     StructField(\"author.playtime_last_two_weeks\", IntegerType(), False),\n",
    "     StructField(\"author.playtime_at_review\", IntegerType(), False),\n",
    "     StructField(\"author.last_played\", IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97c13913-a6b9-456a-81b5-8900d78881b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea85cdb5-2beb-4098-b7c8-c6b25009d8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "    \n",
    "for app in appidList:\n",
    "    \n",
    "    df = spark.read.parquet('s3a://silver/' + app['silverPath'], multiLine=True, header=True, schema=reviews_schema) \n",
    "    \n",
    "    df = df.withColumn(\"appid\", lit(app['appid']))\n",
    "    \n",
    "    df3 = df2.drop_duplicates()\n",
    "    \n",
    "    df4 = df3.select('appid', 'app_name', 'review_id')\n",
    "    \n",
    "    try:\n",
    "        maxReviewID = df4.groupby().max('review_id').first().asDict()['max(review_id)']\n",
    "    except:\n",
    "        maxReviewID = 0\n",
    "    \n",
    "    try:\n",
    "        game_name = df4.select('app_name').distinct().toPandas() \n",
    "    except:\n",
    "        game_name = 'Não identificado'\n",
    "    \n",
    "    \n",
    "    if len(game_name['app_name'][0]) <= 50:\n",
    "        game_name = game_name['app_name'][0]\n",
    "        \n",
    "    else:\n",
    "        game_name = 'Não identificado'\n",
    "    \n",
    "    update_last_review(game_id=app['appid'], game_name=game_name, last_review_retrieved=maxReviewID)  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
